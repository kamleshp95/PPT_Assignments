{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `1. Data Ingestion Pipeline:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To design a data ingestion pipeline in Python that collects and stores data from various sources, such as databases, APIs, and streaming platforms, you can follow these general steps:\n",
    "\n",
    "1. Identify Data Sources:\n",
    "- Determine the data sources you want to collect from, such as databases, APIs, or streaming platforms.\n",
    "- Understand the data formats, access methods, and any authentication or authorization requirements for each source.\n",
    "\n",
    "2. Choose Data Collection Tools and Libraries:\n",
    "- Select appropriate Python libraries and tools that can interact with the different data sources.\n",
    "- For databases, you can use libraries like SQLAlchemy, psycopg2, or pymongo, depending on the database type.\n",
    "- For APIs, libraries like requests or specialized API wrappers can be used.\n",
    "- For streaming platforms, libraries like Kafka-Python, Pulsar, or Apache Beam can be utilized.\n",
    "\n",
    "3. Establish Connection and Authentication:\n",
    "- Set up connections to the data sources using appropriate connection parameters or credentials.\n",
    "- Configure authentication methods, such as API keys or OAuth tokens, as required by the data sources.\n",
    "\n",
    "4. Fetch and Collect Data:\n",
    "- Write functions or classes to fetch data from each source, using the corresponding libraries.\n",
    "- For databases, you can write SQL queries or use ORM (Object-Relational Mapping) techniques.\n",
    "- For APIs, make HTTP requests and process the responses to extract the required data.\n",
    "- For streaming platforms, set up consumer or subscriber clients to consume the data stream.\n",
    "\n",
    "5. Handle Data Transformations and Preprocessing:\n",
    "- Perform any necessary data transformations or preprocessing steps to clean, format, or enrich the collected data.\n",
    "- Use appropriate libraries for data manipulation, cleaning, and transformation, such as pandas or NumPy.\n",
    "\n",
    "6. Define Storage Mechanisms:\n",
    "- Determine the storage mechanisms based on your requirements, such as databases, data lakes, or file systems.\n",
    "- Choose suitable storage technologies like PostgreSQL, MySQL, MongoDB, Apache Hadoop, Apache Parquet, or Amazon S3.\n",
    "\n",
    "7. Write Data to Storage:\n",
    "- Develop code to write the collected and processed data to the chosen storage mechanisms.\n",
    "- Utilize appropriate libraries or database connectors to insert or write the data.\n",
    "- Ensure data integrity, consistency, and error handling during the writing process.\n",
    "\n",
    "8. Implement Scheduling and Automation:\n",
    "- Set up scheduling mechanisms, such as cron jobs or task schedulers, to automate the data ingestion pipeline.\n",
    "- Determine the frequency of data collection and define the intervals or triggers accordingly.\n",
    "\n",
    "9. Implement Error Handling and Logging:\n",
    "- Include error handling mechanisms to handle exceptions or failures during data collection or storage.\n",
    "- Use logging frameworks, such as Python's built-in logging module or third-party libraries like loguru or structlog, to log pipeline activities, errors, and information.\n",
    "\n",
    "10. Monitor and Maintain:\n",
    "- Monitor the data ingestion pipeline for performance, data quality, and any potential issues.\n",
    "- Implement monitoring and alerting mechanisms to identify and address any pipeline failures or anomalies.\n",
    "- Regularly review and maintain the pipeline to adapt to changes in data sources or requirements.\n",
    "\n",
    "Remember, the specific implementation details and libraries used may vary depending on the exact data sources, storage mechanisms, and requirements of your data ingestion pipeline.\n",
    "\n",
    "Here's an example Python code snippet that demonstrates the data ingestion pipeline for collecting and storing data from various sources:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pymongo\n",
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "db_conn = psycopg2.connect(database=\"your_database\", user=\"your_username\", password=\"your_password\", host=\"localhost\", port=\"5432\")\n",
    "db_cursor = db_conn.cursor()\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "mongo_db = mongo_client[\"your_database\"]\n",
    "mongo_collection = mongo_db[\"your_collection\"]\n",
    "\n",
    "# Create an engine for SQLAlchemy\n",
    "db_engine = create_engine('postgresql://your_username:your_password@localhost:5432/your_database')\n",
    "\n",
    "# Connect to Kafka\n",
    "consumer = KafkaConsumer('your_topic', bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "# Fetch data from API\n",
    "response = requests.get('your_api_url')\n",
    "api_data = json.loads(response.text)\n",
    "\n",
    "# Process and store the API data\n",
    "processed_api_data = process_api_data(api_data)\n",
    "db_cursor.execute(\"INSERT INTO your_table (column1, column2) VALUES (%s, %s)\", (processed_api_data['value1'], processed_api_data['value2']))\n",
    "db_conn.commit()\n",
    "\n",
    "# Fetch and process data from Kafka\n",
    "for message in consumer:\n",
    "    kafka_data = json.loads(message.value)\n",
    "    processed_kafka_data = process_kafka_data(kafka_data)\n",
    "    mongo_collection.insert_one(processed_kafka_data)\n",
    "\n",
    "# Fetch data from a database table\n",
    "query = \"SELECT * FROM your_table\"\n",
    "df = pd.read_sql_query(query, db_engine)\n",
    "\n",
    "# Perform data transformations and preprocessing\n",
    "transformed_data = transform_data(df)\n",
    "\n",
    "# Store the transformed data in a file\n",
    "transformed_data.to_csv('transformed_data.csv', index=False)\n",
    "\n",
    "# Close database connections and Kafka consumer\n",
    "db_cursor.close()\n",
    "db_conn.close()\n",
    "mongo_client.close()\n",
    "consumer.close()\n",
    "```\n",
    "\n",
    "Please note that this is just a basic example to give you an idea of how the data ingestion pipeline can be implemented in Python. You would need to customize and expand this code according to your specific requirements, including proper error handling, authentication, and other necessary components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a real-time data ingestion pipeline for processing sensor data from IoT devices in Python, you can utilize a combination of technologies such as MQTT (Message Queuing Telemetry Transport) protocol, MQTT broker, and a Python MQTT client library. Here's an example code snippet that demonstrates the basic implementation of such a pipeline:\n",
    "\n",
    "```python\n",
    "import paho.mqtt.client as mqtt\n",
    "import json\n",
    "import time\n",
    "\n",
    "# MQTT broker settings\n",
    "broker_address = \"mqtt_broker_address\"\n",
    "broker_port = 1883\n",
    "topic = \"your_topic\"\n",
    "\n",
    "# Define callback functions for MQTT events\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    print(\"Connected to MQTT broker with result code: \" + str(rc))\n",
    "    client.subscribe(topic)\n",
    "\n",
    "def on_message(client, userdata, msg):\n",
    "    payload = msg.payload.decode(\"utf-8\")\n",
    "    data = json.loads(payload)\n",
    "    \n",
    "    # Process and analyze the received sensor data\n",
    "    process_sensor_data(data)\n",
    "\n",
    "# Create an MQTT client instance\n",
    "client = mqtt.Client()\n",
    "\n",
    "# Set MQTT event callbacks\n",
    "client.on_connect = on_connect\n",
    "client.on_message = on_message\n",
    "\n",
    "# Connect to the MQTT broker\n",
    "client.connect(broker_address, broker_port, 60)\n",
    "\n",
    "# Start the MQTT client loop to handle incoming messages\n",
    "client.loop_start()\n",
    "\n",
    "# Continuously process sensor data until interrupted\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted, stopping data ingestion.\")\n",
    "\n",
    "# Disconnect from the MQTT broker\n",
    "client.disconnect()\n",
    "```\n",
    "\n",
    "In the above code snippet, we're using the Paho MQTT client library (`paho.mqtt.client`) to connect to an MQTT broker, subscribe to a specific topic, and receive real-time sensor data messages from IoT devices. The `on_connect` and `on_message` callback functions handle the corresponding MQTT events. The `on_message` function processes and analyzes the received sensor data using the `process_sensor_data` function (which you can define as per your requirements).\n",
    "\n",
    "To utilize this code, you need to replace `\"mqtt_broker_address\"` with the actual address of your MQTT broker, update the `\"your_topic\"` placeholder with the desired topic to subscribe to, and implement the `process_sensor_data` function to handle the received data.\n",
    "\n",
    "Remember to install the `paho-mqtt` library before running the code. You can install it via pip using the command: `pip install paho-mqtt`.\n",
    "\n",
    "Additionally, you may need to handle authentication, encryption, and other security aspects based on the MQTT broker configuration and requirements of your IoT infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop a data ingestion pipeline in Python that handles data from different file formats (such as CSV, JSON) and performs data validation and cleansing, you can utilize libraries like `pandas` and `json`. Here's an example code snippet that demonstrates the basic implementation of such a pipeline:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to read and process CSV files\n",
    "def process_csv_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Perform data validation and cleansing operations on the DataFrame\n",
    "        # ...\n",
    "        \n",
    "        # Save the cleaned data to a new file or perform further processing\n",
    "        df.to_csv('cleaned_data.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV file: {str(e)}\")\n",
    "\n",
    "# Function to read and process JSON files\n",
    "def process_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path) as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        # Perform data validation and cleansing operations on the JSON data\n",
    "        # ...\n",
    "        \n",
    "        # Save the cleaned data to a new file or perform further processing\n",
    "        with open('cleaned_data.json', 'w') as json_output:\n",
    "            json.dump(data, json_output)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON file: {str(e)}\")\n",
    "\n",
    "# File paths for example CSV and JSON files\n",
    "csv_file_path = 'example.csv'\n",
    "json_file_path = 'example.json'\n",
    "\n",
    "# Process CSV file\n",
    "process_csv_file(csv_file_path)\n",
    "\n",
    "# Process JSON file\n",
    "process_json_file(json_file_path)\n",
    "```\n",
    "\n",
    "In the above code snippet, we define two functions, `process_csv_file` and `process_json_file`, to handle CSV and JSON files, respectively. These functions use `pandas` and `json` libraries to read the files and perform data validation and cleansing operations.\n",
    "\n",
    "Inside the functions, you can add your specific data validation and cleansing logic to handle missing values, incorrect data types, outliers, or any other necessary checks. Once the data is validated and cleaned, you can save it to a new file or perform further processing as per your requirements.\n",
    "\n",
    "To use this code, replace the `csv_file_path` and `json_file_path` variables with the actual file paths of your CSV and JSON files, respectively. Also, make sure you have the necessary libraries (`pandas` and `json`) installed before running the code.\n",
    "\n",
    "Remember to adapt the code based on the specific data validation and cleansing operations you need to perform on your files. This example provides a starting point, and you can modify and expand it according to your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `2. Model Training:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Build a machine learning model to predict customer churn based on a given dataset. Train the model using appropriate algorithms and evaluate its performance.<br>\n",
    "## b. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, feature scaling, and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\").drop(\"customerID\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  Female              0     Yes         No       1           No   \n",
       "1    Male              0      No         No      34          Yes   \n",
       "2    Male              0      No         No       2          Yes   \n",
       "3    Male              0      No         No      45           No   \n",
       "4  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity OnlineBackup  \\\n",
       "0  No phone service             DSL             No          Yes   \n",
       "1                No             DSL            Yes           No   \n",
       "2                No             DSL            Yes          Yes   \n",
       "3  No phone service             DSL            Yes           No   \n",
       "4                No     Fiber optic             No           No   \n",
       "\n",
       "  DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
       "0               No          No          No              No  Month-to-month   \n",
       "1              Yes          No          No              No        One year   \n",
       "2               No          No          No              No  Month-to-month   \n",
       "3              Yes         Yes          No              No        One year   \n",
       "4               No          No          No              No  Month-to-month   \n",
       "\n",
       "  PaperlessBilling              PaymentMethod  MonthlyCharges TotalCharges  \\\n",
       "0              Yes           Electronic check           29.85        29.85   \n",
       "1               No               Mailed check           56.95       1889.5   \n",
       "2              Yes               Mailed check           53.85       108.15   \n",
       "3               No  Bank transfer (automatic)           42.30      1840.75   \n",
       "4              Yes           Electronic check           70.70       151.65   \n",
       "\n",
       "  Churn  \n",
       "0    No  \n",
       "1    No  \n",
       "2   Yes  \n",
       "3    No  \n",
       "4   Yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   gender            7043 non-null   object \n",
      " 1   SeniorCitizen     7043 non-null   int64  \n",
      " 2   Partner           7043 non-null   object \n",
      " 3   Dependents        7043 non-null   object \n",
      " 4   tenure            7043 non-null   int64  \n",
      " 5   PhoneService      7043 non-null   object \n",
      " 6   MultipleLines     7043 non-null   object \n",
      " 7   InternetService   7043 non-null   object \n",
      " 8   OnlineSecurity    7043 non-null   object \n",
      " 9   OnlineBackup      7043 non-null   object \n",
      " 10  DeviceProtection  7043 non-null   object \n",
      " 11  TechSupport       7043 non-null   object \n",
      " 12  StreamingTV       7043 non-null   object \n",
      " 13  StreamingMovies   7043 non-null   object \n",
      " 14  Contract          7043 non-null   object \n",
      " 15  PaperlessBilling  7043 non-null   object \n",
      " 16  PaymentMethod     7043 non-null   object \n",
      " 17  MonthlyCharges    7043 non-null   float64\n",
      " 18  TotalCharges      7043 non-null   object \n",
      " 19  Churn             7043 non-null   object \n",
      "dtypes: float64(1), int64(2), object(17)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert string to float: ''\n",
      "488\n",
      "could not convert string to float: ''\n",
      "753\n",
      "could not convert string to float: ''\n",
      "936\n",
      "could not convert string to float: ''\n",
      "1082\n",
      "could not convert string to float: ''\n",
      "1340\n",
      "could not convert string to float: ''\n",
      "3331\n",
      "could not convert string to float: ''\n",
      "3826\n",
      "could not convert string to float: ''\n",
      "4380\n",
      "could not convert string to float: ''\n",
      "5218\n",
      "could not convert string to float: ''\n",
      "6670\n",
      "could not convert string to float: ''\n",
      "6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1h/_nq6ykn97db9yg5gh07fg8240000gn/T/ipykernel_2272/3042339562.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"TotalCharges\"].iloc[i] = \"0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    try:\n",
    "        float(df[\"TotalCharges\"].iloc[i])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(i)\n",
    "        df[\"TotalCharges\"].iloc[i] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(0 , np.mean(df[\"TotalCharges\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender              0\n",
       "SeniorCitizen       0\n",
       "Partner             0\n",
       "Dependents          0\n",
       "tenure              0\n",
       "PhoneService        0\n",
       "MultipleLines       0\n",
       "InternetService     0\n",
       "OnlineSecurity      0\n",
       "OnlineBackup        0\n",
       "DeviceProtection    0\n",
       "TechSupport         0\n",
       "StreamingTV         0\n",
       "StreamingMovies     0\n",
       "Contract            0\n",
       "PaperlessBilling    0\n",
       "PaymentMethod       0\n",
       "MonthlyCharges      0\n",
       "TotalCharges        0\n",
       "Churn               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Churn = df.Churn.map({\"No\":0,\"Yes\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=\"object\").columns\n",
    "numerical_cols = df.select_dtypes(exclude='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gender': array(['Female', 'Male'], dtype=object)}\n",
      "{'Partner': array(['Yes', 'No'], dtype=object)}\n",
      "{'Dependents': array(['No', 'Yes'], dtype=object)}\n",
      "{'PhoneService': array(['No', 'Yes'], dtype=object)}\n",
      "{'MultipleLines': array(['No phone service', 'No', 'Yes'], dtype=object)}\n",
      "{'InternetService': array(['DSL', 'Fiber optic', 'No'], dtype=object)}\n",
      "{'OnlineSecurity': array(['No', 'Yes', 'No internet service'], dtype=object)}\n",
      "{'OnlineBackup': array(['Yes', 'No', 'No internet service'], dtype=object)}\n",
      "{'DeviceProtection': array(['No', 'Yes', 'No internet service'], dtype=object)}\n",
      "{'TechSupport': array(['No', 'Yes', 'No internet service'], dtype=object)}\n",
      "{'StreamingTV': array(['No', 'Yes', 'No internet service'], dtype=object)}\n",
      "{'StreamingMovies': array(['No', 'Yes', 'No internet service'], dtype=object)}\n",
      "{'Contract': array(['Month-to-month', 'One year', 'Two year'], dtype=object)}\n",
      "{'PaperlessBilling': array(['Yes', 'No'], dtype=object)}\n",
      "{'PaymentMethod': array(['Electronic check', 'Mailed check', 'Bank transfer (automatic)',\n",
      "       'Credit card (automatic)'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "for i in df[categorical_cols]:\n",
    "    print({i:df[i].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = numerical_cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent features and independent features\n",
    "X = df.drop('Churn', axis=1)\n",
    "y =df.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder , StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import  Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test  = train_test_split(X,y,test_size=0.3 , random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = Pipeline(\n",
    "    steps = [\n",
    "        ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder',OneHotEncoder(drop='if_binary'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "numerical_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical_pipeline',categorical_pipeline , categorical_cols),\n",
    "    ('numerical_pipeline',numerical_pipeline , numerical_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  pd.DataFrame(preprocessor.fit_transform(X_train),columns=preprocessor.get_feature_names_out())\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test),columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('pca', PCA(n_components=5)),\n",
    "        ('clf' , RandomForestClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train , y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "report = classification_report(y_test , y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      1560\n",
      "           1       0.57      0.45      0.50       553\n",
      "\n",
      "    accuracy                           0.77      2113\n",
      "   macro avg       0.70      0.67      0.68      2113\n",
      "weighted avg       0.75      0.77      0.76      2113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Train a deep learning model for image classification using transfer learning and fine-tuning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example code snippet that demonstrates training a deep learning model for image classification using transfer learning and fine-tuning techniques in Python, using the Keras library with the TensorFlow backend:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model's layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define data generators for training and validation data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('train_directory', target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('validation_directory', target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=10, validation_data=val_generator, validation_steps=len(val_generator))\n",
    "\n",
    "# Fine-tune the model by unfreezing some layers\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=5, validation_data=val_generator, validation_steps=len(val_generator))\n",
    "```\n",
    "\n",
    "In the above code, we utilize the VGG16 pre-trained model as the base model and add a few additional layers on top to create a new model for image classification. We freeze the layers of the pre-trained base model and train only the additional layers initially. Then, we fine-tune the model by unfreezing some layers of the base model and retraining the entire model.\n",
    "\n",
    "To use this code, make sure to replace `'train_directory'` and `'validation_directory'` with the actual directories containing your training and validation image datasets, respectively. The directory structure should follow the standard format for Keras' `flow_from_directory` method, where each class of images resides in a separate subdirectory.\n",
    "\n",
    "Additionally, you may need to adjust parameters, such as learning rates, batch sizes, number of classes, and other hyperparameters, according to your specific requirements.\n",
    "\n",
    "Remember to have the necessary dependencies installed, such as TensorFlow and Keras, before running the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `3. Model Validation:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## a. Implement cross-validation to evaluate the performance of a regression model for predicting housing prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import  SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV , train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the boston housing dataset\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test  = train_test_split(data,target,test_size=0.3,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss':['squared_error', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive'],\n",
    "    'penalty':['l1','l2','elasticnet'],\n",
    "    'alpha' : [0.00001,0.0001,0.001,0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV 1/5] END alpha=1e-05, loss=squared_error, penalty=l1;, score=-89853427907556.703 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=squared_error, penalty=l1;, score=-329800920510864.000 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=squared_error, penalty=l1;, score=-51729516728226.945 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=squared_error, penalty=l1;, score=-115317722340204.469 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=squared_error, penalty=l1;, score=-80330939598384.344 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=squared_error, penalty=l2;, score=-48469219125899.406 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=squared_error, penalty=l2;, score=-97091790511809.859 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=squared_error, penalty=l2;, score=-230582313754804.188 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=squared_error, penalty=l2;, score=-118349530102800.219 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=squared_error, penalty=l2;, score=-203004820065437.812 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=squared_error, penalty=elasticnet;, score=-110049365034191.078 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=squared_error, penalty=elasticnet;, score=-97182540035781.625 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=squared_error, penalty=elasticnet;, score=-138105255645010.906 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=squared_error, penalty=elasticnet;, score=-164158731311153.406 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=squared_error, penalty=elasticnet;, score=-54708865846967.977 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=huber, penalty=l1;, score=-9.587 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=huber, penalty=l1;, score=-9.010 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=huber, penalty=l1;, score=-9.831 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=huber, penalty=l1;, score=-9.777 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=huber, penalty=l1;, score=-16.641 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=huber, penalty=l2;, score=-26.147 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=huber, penalty=l2;, score=-18.445 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=huber, penalty=l2;, score=-8.545 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=huber, penalty=l2;, score=-24.970 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=huber, penalty=l2;, score=-10.591 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=huber, penalty=elasticnet;, score=-20.621 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=huber, penalty=elasticnet;, score=-38.511 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=huber, penalty=elasticnet;, score=-11.240 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=huber, penalty=elasticnet;, score=-17.116 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=huber, penalty=elasticnet;, score=-16.094 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l1;, score=-165.188 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l1;, score=-104.671 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l1;, score=-211.498 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l1;, score=-192.729 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l1;, score=-105.159 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l2;, score=-117.906 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l2;, score=-84.457 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l2;, score=-73.431 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l2;, score=-127.223 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=l2;, score=-54.733 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=elasticnet;, score=-111.701 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=elasticnet;, score=-139.335 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=elasticnet;, score=-284.792 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=elasticnet;, score=-89.298 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=epsilon_insensitive, penalty=elasticnet;, score=-185.909 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l1;, score=-75524126475043.062 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l1;, score=-142712460924183.500 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l1;, score=-169050349528865.094 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l1;, score=-90601185245449.297 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l1;, score=-38625584011439.453 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l2;, score=-129659523960150.688 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l2;, score=-116195195960817.312 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l2;, score=-57550324253976.742 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l2;, score=-227944092070987.594 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=l2;, score=-318230278080964.625 total time=   0.0s\n",
      "[CV 1/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-217158958045663.000 total time=   0.0s\n",
      "[CV 2/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-15525689278873.154 total time=   0.0s\n",
      "[CV 3/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-52804427705153.086 total time=   0.0s\n",
      "[CV 4/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-149796658312547.562 total time=   0.0s\n",
      "[CV 5/5] END alpha=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-105380370283531.109 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=squared_error, penalty=l1;, score=-58093879167926.539 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=squared_error, penalty=l1;, score=-122850066232506.109 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=squared_error, penalty=l1;, score=-139274698702475.625 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=squared_error, penalty=l1;, score=-70223896683304.570 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=squared_error, penalty=l1;, score=-79757147266344.625 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=squared_error, penalty=l2;, score=-191962001056713.719 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=squared_error, penalty=l2;, score=-136427511106773.672 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=squared_error, penalty=l2;, score=-58692850350750.016 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=squared_error, penalty=l2;, score=-44368168940165.938 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=squared_error, penalty=l2;, score=-103805980695434.203 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=squared_error, penalty=elasticnet;, score=-29623611260713.223 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=squared_error, penalty=elasticnet;, score=-52201760860134.258 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=squared_error, penalty=elasticnet;, score=-183289348141538.781 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=squared_error, penalty=elasticnet;, score=-153689688122104.312 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=squared_error, penalty=elasticnet;, score=-60680896869163.211 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=huber, penalty=l1;, score=-11.258 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=huber, penalty=l1;, score=-15.142 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=huber, penalty=l1;, score=-20.973 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=huber, penalty=l1;, score=-13.905 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=huber, penalty=l1;, score=-18.755 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=huber, penalty=l2;, score=-21.153 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=huber, penalty=l2;, score=-17.557 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=huber, penalty=l2;, score=-8.848 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=huber, penalty=l2;, score=-34.528 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=huber, penalty=l2;, score=-30.456 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=huber, penalty=elasticnet;, score=-8.760 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=huber, penalty=elasticnet;, score=-15.634 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=huber, penalty=elasticnet;, score=-8.957 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=huber, penalty=elasticnet;, score=-28.441 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=huber, penalty=elasticnet;, score=-35.637 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l1;, score=-31.329 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l1;, score=-143.753 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l1;, score=-38.630 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l1;, score=-84.589 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l1;, score=-155.803 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l2;, score=-150.785 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l2;, score=-213.300 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l2;, score=-91.641 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l2;, score=-130.195 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=l2;, score=-111.924 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=elasticnet;, score=-165.360 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=elasticnet;, score=-252.898 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=elasticnet;, score=-173.392 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=elasticnet;, score=-112.667 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=epsilon_insensitive, penalty=elasticnet;, score=-211.483 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l1;, score=-147681180521339.594 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l1;, score=-138363046284662.672 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l1;, score=-284969534715836.625 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l1;, score=-328238474702498.500 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l1;, score=-103716054752680.125 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l2;, score=-50582280503540.141 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l2;, score=-161572080613162.250 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l2;, score=-240342484597534.625 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l2;, score=-186027636414551.656 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=l2;, score=-260345536158720.906 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-79628589196045.375 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-82154398023851.016 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-122326402366755.078 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-86689593731421.766 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-183492811236722.094 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=squared_error, penalty=l1;, score=-160165294079453.062 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=squared_error, penalty=l1;, score=-158296496782120.500 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=squared_error, penalty=l1;, score=-42757587014644.258 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=squared_error, penalty=l1;, score=-53176799197218.609 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=squared_error, penalty=l1;, score=-193567868874932.156 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=squared_error, penalty=l2;, score=-146812777995068.094 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=squared_error, penalty=l2;, score=-232424109644230.031 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=squared_error, penalty=l2;, score=-195963643095689.875 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=squared_error, penalty=l2;, score=-344411688278271.312 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=squared_error, penalty=l2;, score=-239022196924301.250 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=squared_error, penalty=elasticnet;, score=-139388786503814.078 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=squared_error, penalty=elasticnet;, score=-333345588776895.688 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=squared_error, penalty=elasticnet;, score=-225865338335561.406 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=squared_error, penalty=elasticnet;, score=-150902041829238.125 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=squared_error, penalty=elasticnet;, score=-58244374195550.977 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=huber, penalty=l1;, score=-9.312 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=huber, penalty=l1;, score=-13.901 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=huber, penalty=l1;, score=-8.796 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=huber, penalty=l1;, score=-21.231 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=huber, penalty=l1;, score=-19.256 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=huber, penalty=l2;, score=-10.517 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=huber, penalty=l2;, score=-36.511 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=huber, penalty=l2;, score=-26.960 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=huber, penalty=l2;, score=-36.842 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=huber, penalty=l2;, score=-8.880 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=huber, penalty=elasticnet;, score=-29.852 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=huber, penalty=elasticnet;, score=-8.798 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=huber, penalty=elasticnet;, score=-10.044 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=huber, penalty=elasticnet;, score=-15.024 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=huber, penalty=elasticnet;, score=-28.319 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l1;, score=-78.180 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l1;, score=-182.685 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l1;, score=-38.967 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l1;, score=-55.275 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l1;, score=-58.488 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l2;, score=-95.027 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l2;, score=-86.668 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l2;, score=-107.035 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l2;, score=-91.469 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=epsilon_insensitive, penalty=l2;, score=-110.088 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=epsilon_insensitive, penalty=elasticnet;, score=-71.399 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=epsilon_insensitive, penalty=elasticnet;, score=-110.084 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=epsilon_insensitive, penalty=elasticnet;, score=-13.593 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=epsilon_insensitive, penalty=elasticnet;, score=-155.725 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=epsilon_insensitive, penalty=elasticnet;, score=-18.101 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l1;, score=-16646510262366.121 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l1;, score=-99141155138967.562 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l1;, score=-218114862754239.812 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l1;, score=-241790637221040.781 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l1;, score=-21183630471846.480 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l2;, score=-111419161656425.812 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l2;, score=-67193112678926.258 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l2;, score=-185849751961739.875 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l2;, score=-135764407293285.125 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=l2;, score=-25297776891682.230 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-222182314383139.781 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-146221320627589.312 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-206897642013151.594 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-175802433167052.656 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-83907561505065.609 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=squared_error, penalty=l1;, score=-297782777589283.625 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=squared_error, penalty=l1;, score=-227719197194042.875 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=squared_error, penalty=l1;, score=-215063078414708.156 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=squared_error, penalty=l1;, score=-294552792916466.438 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=squared_error, penalty=l1;, score=-173384309314636.906 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=squared_error, penalty=l2;, score=-177832602761673.906 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=squared_error, penalty=l2;, score=-68432855325125.555 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=squared_error, penalty=l2;, score=-42820620325270.242 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=squared_error, penalty=l2;, score=-303927502495460.375 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=squared_error, penalty=l2;, score=-89055514874363.219 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=squared_error, penalty=elasticnet;, score=-271394781966888.750 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=squared_error, penalty=elasticnet;, score=-55188731190365.055 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=squared_error, penalty=elasticnet;, score=-102004916536783.969 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=squared_error, penalty=elasticnet;, score=-175732594694300.625 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=squared_error, penalty=elasticnet;, score=-44796393288864.633 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=huber, penalty=l1;, score=-12.688 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=huber, penalty=l1;, score=-7.179 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=huber, penalty=l1;, score=-8.823 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=huber, penalty=l1;, score=-28.553 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=huber, penalty=l1;, score=-10.557 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=huber, penalty=l2;, score=-7.828 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=huber, penalty=l2;, score=-16.782 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=huber, penalty=l2;, score=-15.808 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=huber, penalty=l2;, score=-17.270 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=huber, penalty=l2;, score=-13.708 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=huber, penalty=elasticnet;, score=-33.478 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=huber, penalty=elasticnet;, score=-28.515 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=huber, penalty=elasticnet;, score=-11.114 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=huber, penalty=elasticnet;, score=-17.978 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=huber, penalty=elasticnet;, score=-17.844 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l1;, score=-214.098 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l1;, score=-63.497 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l1;, score=-180.217 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l1;, score=-159.317 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l1;, score=-26.740 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l2;, score=-164.466 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l2;, score=-195.339 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l2;, score=-175.922 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l2;, score=-161.799 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=epsilon_insensitive, penalty=l2;, score=-60.437 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=epsilon_insensitive, penalty=elasticnet;, score=-33.183 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=epsilon_insensitive, penalty=elasticnet;, score=-172.108 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=epsilon_insensitive, penalty=elasticnet;, score=-161.195 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=epsilon_insensitive, penalty=elasticnet;, score=-96.658 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=epsilon_insensitive, penalty=elasticnet;, score=-221.093 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l1;, score=-158423629309285.406 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l1;, score=-102037569988288.516 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l1;, score=-168576643578436.500 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l1;, score=-259909558245897.219 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l1;, score=-89022238936185.984 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l2;, score=-62172988207397.250 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l2;, score=-264117810340662.281 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l2;, score=-97811082254255.688 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l2;, score=-35690191440012.055 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=l2;, score=-226093354583383.500 total time=   0.0s\n",
      "[CV 1/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-251593402826432.812 total time=   0.0s\n",
      "[CV 2/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-176691462973951.938 total time=   0.0s\n",
      "[CV 3/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-195511927461806.250 total time=   0.0s\n",
      "[CV 4/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-306875700328686.938 total time=   0.0s\n",
      "[CV 5/5] END alpha=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet;, score=-106796574837379.125 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SGDRegressor(),\n",
       "             param_grid={&#x27;alpha&#x27;: [1e-05, 0.0001, 0.001, 0.01],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;huber&#x27;,\n",
       "                                  &#x27;epsilon_insensitive&#x27;,\n",
       "                                  &#x27;squared_epsilon_insensitive&#x27;],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;]},\n",
       "             scoring=&#x27;neg_root_mean_squared_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SGDRegressor(),\n",
       "             param_grid={&#x27;alpha&#x27;: [1e-05, 0.0001, 0.001, 0.01],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;huber&#x27;,\n",
       "                                  &#x27;epsilon_insensitive&#x27;,\n",
       "                                  &#x27;squared_epsilon_insensitive&#x27;],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;]},\n",
       "             scoring=&#x27;neg_root_mean_squared_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SGDRegressor(),\n",
       "             param_grid={'alpha': [1e-05, 0.0001, 0.001, 0.01],\n",
       "                         'loss': ['squared_error', 'huber',\n",
       "                                  'epsilon_insensitive',\n",
       "                                  'squared_epsilon_insensitive'],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet']},\n",
       "             scoring='neg_root_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = SGDRegressor()\n",
    "reg = GridSearchCV(regressor ,param_grid=params , cv=5 , scoring= 'neg_root_mean_squared_error',verbose=3 )\n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(alpha=1e-05, loss=&#x27;huber&#x27;, penalty=&#x27;l1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(alpha=1e-05, loss=&#x27;huber&#x27;, penalty=&#x27;l1&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(alpha=1e-05, loss='huber', penalty='l1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.06711802186344\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already answered as a part of Question no. 2.a and 2.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_breast_cancer()\n",
    "X = dataset['data']\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.9455674584691819\n",
      "Average Precision: 0.9460966289135303\n",
      "Average Recall: 0.9692097026604071\n",
      "Average F1-score: 0.9572072923626205\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score \n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "# Create an instance of StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform cross-validation using stratified sampling\n",
    "for train_idx, val_idx in kfold.split(X, y):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train and evaluate your model on the current fold\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "    # Store the evaluation metrics for the current fold\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average performance metrics across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print the average performance metrics\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1-score:\", avg_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `4. Deployment Strategy:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete implementation of a deployment strategy for a machine learning model providing real-time recommendations based on user interactions involves multiple components and infrastructure setup. Below is an example Python code snippet that demonstrates the basic structure of the recommendation generation part:\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Define a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Function to generate recommendations based on user interactions\n",
    "def generate_recommendations(user_interaction):\n",
    "    # Apply preprocessing and feature engineering steps to user_interaction\n",
    "    preprocessed_data = preprocess(user_interaction)\n",
    "\n",
    "    # Pass the preprocessed data through the deployed model\n",
    "    recommendations = model.predict(preprocessed_data)\n",
    "\n",
    "    # Apply post-processing and filtering steps to recommendations\n",
    "    filtered_recommendations = post_process(recommendations)\n",
    "\n",
    "    return filtered_recommendations\n",
    "\n",
    "# Example function to preprocess user interactions\n",
    "def preprocess(user_interaction):\n",
    "    # Apply necessary preprocessing steps such as encoding, scaling, etc.\n",
    "    preprocessed_data = ...  # Preprocessed data ready for model input\n",
    "    return preprocessed_data\n",
    "\n",
    "# Example function for post-processing and filtering recommendations\n",
    "def post_process(recommendations):\n",
    "    # Apply necessary post-processing and filtering steps\n",
    "    filtered_recommendations = ...  # Filtered recommendations ready for response\n",
    "    return filtered_recommendations\n",
    "\n",
    "# Example code for real-time recommendation generation\n",
    "while True:\n",
    "    try:\n",
    "        # Simulate receiving user interaction data in real-time\n",
    "        user_interaction = receive_user_interaction()\n",
    "\n",
    "        # Generate recommendations based on user interaction\n",
    "        recommendations = generate_recommendations(user_interaction)\n",
    "\n",
    "        # Send recommendations to the user-facing application or API\n",
    "        send_recommendations(recommendations)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in real-time recommendation generation: %s\", str(e))\n",
    "\n",
    "    # Sleep for a certain period before processing the next user interaction\n",
    "    time.sleep(1)\n",
    "```\n",
    "\n",
    "Please note that this is a simplified code snippet meant to illustrate the logic of recommendation generation in a real-time scenario. The implementation of `preprocess`, `post_process`, `receive_user_interaction`, and `send_recommendations` functions should be tailored to your specific requirements and infrastructure.\n",
    "\n",
    "In a complete implementation, you would need to set up the necessary infrastructure for real-time data ingestion, data transformation, model serving, logging, and deployment orchestration. Additionally, you'll need to consider scalability, error handling, security, and performance optimization based on your specific deployment environment and requirements.\n",
    "\n",
    "Remember to adapt the code to your specific model, data preprocessing, and post-processing needs, as well as integrate it with your existing infrastructure and deployment pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying machine learning models to cloud platforms like AWS or Azure typically involves several steps, including model packaging, infrastructure provisioning, and deployment automation. Here's an example of a Python code snippet that demonstrates the basic structure of a deployment pipeline using AWS as the cloud platform:\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Set AWS credentials and region\n",
    "AWS_ACCESS_KEY_ID = 'your-access-key'\n",
    "AWS_SECRET_ACCESS_KEY = 'your-secret-access-key'\n",
    "AWS_REGION = 'your-aws-region'\n",
    "\n",
    "# Set model and deployment parameters\n",
    "MODEL_NAME = 'your-model-name'\n",
    "DEPLOYMENT_NAME = 'your-deployment-name'\n",
    "BUCKET_NAME = 'your-s3-bucket-name'\n",
    "ZIP_FILE_NAME = 'model.zip'\n",
    "ENTRY_FILE_NAME = 'app.py'\n",
    "\n",
    "# Create a new AWS S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)\n",
    "\n",
    "# Upload the model file to S3 bucket\n",
    "def upload_model():\n",
    "    s3.upload_file(ZIP_FILE_NAME, BUCKET_NAME, ZIP_FILE_NAME)\n",
    "    print(f\"Uploaded {ZIP_FILE_NAME} to S3 bucket {BUCKET_NAME}\")\n",
    "\n",
    "# Create a new AWS SageMaker client\n",
    "sm = boto3.client('sagemaker', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)\n",
    "\n",
    "# Create a new SageMaker endpoint configuration\n",
    "def create_endpoint_config():\n",
    "    response = sm.create_endpoint_config(\n",
    "        EndpointConfigName=DEPLOYMENT_NAME,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                'VariantName': 'AllTraffic',\n",
    "                'ModelName': MODEL_NAME,\n",
    "                'InitialInstanceCount': 1,\n",
    "                'InstanceType': 'ml.t2.medium',\n",
    "                'InitialVariantWeight': 1\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Created endpoint configuration: {DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Create a new SageMaker endpoint\n",
    "def create_endpoint():\n",
    "    response = sm.create_endpoint(\n",
    "        EndpointName=DEPLOYMENT_NAME,\n",
    "        EndpointConfigName=DEPLOYMENT_NAME\n",
    "    )\n",
    "    print(f\"Created endpoint: {DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Package the model and required files into a zip file\n",
    "def package_model():\n",
    "    with zipfile.ZipFile(ZIP_FILE_NAME, 'w') as zipf:\n",
    "        zipf.write(ENTRY_FILE_NAME)\n",
    "        zipf.write('your_model.pkl')\n",
    "\n",
    "    print(f\"Packaged model and files into {ZIP_FILE_NAME}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "def clean_up():\n",
    "    os.remove(ZIP_FILE_NAME)\n",
    "    print(\"Cleaned up temporary files\")\n",
    "\n",
    "# Main deployment pipeline\n",
    "def deploy():\n",
    "    # Package the model and required files\n",
    "    package_model()\n",
    "\n",
    "    # Upload the model to S3\n",
    "    upload_model()\n",
    "\n",
    "    # Create an endpoint configuration\n",
    "    create_endpoint_config()\n",
    "\n",
    "    # Create an endpoint\n",
    "    create_endpoint()\n",
    "\n",
    "    # Clean up temporary files\n",
    "    clean_up()\n",
    "\n",
    "# Call the main deployment pipeline function\n",
    "deploy()\n",
    "```\n",
    "\n",
    "This code assumes that you have prepared the model file (`your_model.pkl`), the deployment entry file (`app.py`), and the necessary AWS credentials (`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`). Modify the `MODEL_NAME`, `DEPLOYMENT_NAME`, `BUCKET_NAME`, `ZIP_FILE_NAME`, and `ENTRY_FILE_NAME` variables according to your specific requirements.\n",
    "\n",
    "The code uses the AWS SDK for Python (`boto3`) to interact with AWS services. It uploads the model file to an S3 bucket, creates an endpoint configuration, and creates an endpoint for the deployment. It packages the model file and the entry file into a zip file (`model.zip`) for uploading. Once the deployment is complete, it cleans up the temporary files.\n",
    "\n",
    "Please note that this code is a simplified example and should be adapted to your specific deployment requirements, such as customizing the instance type, adding additional files, or setting up specific networking configurations. Additionally, ensure that you have the necessary dependencies (boto3, zipfile) installed before running the code.\n",
    "\n",
    "To deploy the model to Azure or other cloud platforms, you'll need to use the respective SDKs and APIs provided by the platform. The general concept of packaging the model, uploading it to storage, and creating an endpoint or service remains similar, but the specific implementation details will differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing a monitoring and maintenance strategy for deployed machine learning models is crucial to ensure their ongoing performance, reliability, and to detect any issues that may arise. Below is an example of a monitoring and maintenance strategy along with corresponding Python code snippets:\n",
    "\n",
    "1. **Performance Monitoring**:\n",
    "   - Set up monitoring for key performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "   - Calculate and log these metrics periodically to track the model's performance over time.\n",
    "   - Detect any significant changes in performance that may indicate model degradation.\n",
    "   \n",
    "   ```python\n",
    "   import logging\n",
    "   import time\n",
    "   \n",
    "   # Define a logger\n",
    "   logger = logging.getLogger(__name__)\n",
    "   logger.setLevel(logging.INFO)\n",
    "   formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "   console_handler = logging.StreamHandler()\n",
    "   console_handler.setFormatter(formatter)\n",
    "   logger.addHandler(console_handler)\n",
    "   \n",
    "   # Function to calculate and log model performance metrics\n",
    "   def monitor_performance():\n",
    "       # Calculate performance metrics\n",
    "       accuracy = calculate_accuracy()\n",
    "       precision = calculate_precision()\n",
    "       recall = calculate_recall()\n",
    "       f1_score = calculate_f1_score()\n",
    "   \n",
    "       # Log the performance metrics\n",
    "       logger.info(\"Accuracy: %.4f\", accuracy)\n",
    "       logger.info(\"Precision: %.4f\", precision)\n",
    "       logger.info(\"Recall: %.4f\", recall)\n",
    "       logger.info(\"F1-score: %.4f\", f1_score)\n",
    "   \n",
    "   # Example code for periodically monitoring performance\n",
    "   while True:\n",
    "       try:\n",
    "           # Perform performance monitoring\n",
    "           monitor_performance()\n",
    "       except Exception as e:\n",
    "           logger.error(\"Error in performance monitoring: %s\", str(e))\n",
    "   \n",
    "       # Sleep for a specific interval before monitoring again\n",
    "       time.sleep(3600)  # Sleep for 1 hour\n",
    "   ```\n",
    "\n",
    "2. **Data Drift Monitoring**:\n",
    "   - Set up mechanisms to monitor data drift and distributional changes in the input data over time.\n",
    "   - Compare the current data distribution with the training data distribution.\n",
    "   - Detect significant deviations that may impact the model's performance and trigger alerts or retraining.\n",
    "   \n",
    "   ```python\n",
    "   # Function to monitor data drift\n",
    "   def monitor_data_drift():\n",
    "       # Load the training data distribution\n",
    "       training_data_distribution = load_training_data_distribution()\n",
    "   \n",
    "       # Get the current data distribution\n",
    "       current_data_distribution = get_current_data_distribution()\n",
    "   \n",
    "       # Compare the distributions and detect drift\n",
    "       drift_detected = detect_data_drift(training_data_distribution, current_data_distribution)\n",
    "   \n",
    "       # Trigger alert or retraining based on drift detection\n",
    "       if drift_detected:\n",
    "           trigger_alert()\n",
    "           retrain_model()\n",
    "   \n",
    "   # Example code for periodically monitoring data drift\n",
    "   while True:\n",
    "       try:\n",
    "           # Perform data drift monitoring\n",
    "           monitor_data_drift()\n",
    "       except Exception as e:\n",
    "           logger.error(\"Error in data drift monitoring: %s\", str(e))\n",
    "   \n",
    "       # Sleep for a specific interval before monitoring again\n",
    "       time.sleep(3600)  # Sleep for 1 hour\n",
    "   ```\n",
    "\n",
    "3. **Model Health Checks**:\n",
    "   - Regularly evaluate the model's health and integrity by performing sanity checks on input/output data and model predictions.\n",
    "   - Detect anomalies or inconsistencies in the model's behavior and trigger alerts or initiate corrective actions.\n",
    "   \n",
    "   ```python\n",
    "   # Function to perform model health checks\n",
    "   def perform_health_checks():\n",
    "       # Perform checks on input/output data and model predictions\n",
    "       data_consistency_check = check_input_output_consistency()\n",
    "       anomaly_detection_check = check_for_anomalies()\n",
    "   \n",
    "       # Trigger alerts or corrective actions based on health check results\n",
    "       if not data_consistency_check:\n",
    "           trigger_alert(\"Data inconsistency detected\")\n",
    "       if anomaly_detection_check:\n",
    "           initiate_corrective_action()\n",
    "   \n",
    "   # Example code for periodically performing model health checks\n",
    "   while True:\n",
    "       try:\n",
    "           # Perform model health checks\n",
    "           perform_health_checks()\n",
    "       except Exception as e:\n",
    "           logger.error(\"Error in model health checks: %s\", str(e))\n",
    "   \n",
    "       # Sleep for a specific interval before performing health checks again\n",
    "       time.sleep(3600)  # Sleep for 1 hour\n",
    "   ```\n",
    "\n",
    "4. **Model Retraining**:\n",
    "   - Define retraining criteria based on performance degradation, data drift, or predefined schedules.\n",
    "   - Automatically trigger the retraining process when the defined criteria are met.\n",
    "   \n",
    "   ```python\n",
    "   # Function to trigger model retraining\n",
    "   def retrain_model():\n",
    "       # Perform necessary steps for model retraining\n",
    "       data_preparation()\n",
    "       model_training()\n",
    "       model_evaluation()\n",
    "       model_deployment()\n",
    "   \n",
    "   # Example code for periodically checking retraining criteria and triggering retraining\n",
    "   while True:\n",
    "       try:\n",
    "           # Check retraining criteria and trigger retraining if met\n",
    "           if is_retraining_required():\n",
    "               retrain_model()\n",
    "       except Exception as e:\n",
    "           logger.error(\"Error in retraining process: %s\", str(e))\n",
    "   \n",
    "       # Sleep for a specific interval before checking retraining criteria again\n",
    "       time.sleep(86400)  # Sleep for 1 day\n",
    "   ```\n",
    "\n",
    "Remember to adapt the code snippets to fit your specific model, monitoring requirements, and infrastructure. Additionally, integrate the code snippets with appropriate logging mechanisms and alerting systems to ensure timely notifications when issues arise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
